{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporary, for convenience\n",
    "\n",
    "DECISIONS:\n",
    "1. Calculate the drawdown and assign days with drawdown in the 99.5% quantile as crash (i.e. label=1). \n",
    "2. The target is shift 3 days to make the problem as \"Predict if there will be a market crash 3 days later\".\n",
    "\n",
    "\n",
    "\n",
    "To-do: \n",
    "- More Plots?\n",
    "- Add data\n",
    "- more hyperparameter search\n",
    "- better accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and imports   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yfinance as yf # to get our data\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler # to normalize our classes\n",
    "from sklearn.model_selection import train_test_split # from tutorial code\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix # needed for plotting\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Data from Yahoo Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^GSPC S&P 500 ^GSPC (24349, 7) 1927-12-30 00:00:00-05:00 2024-12-04 00:00:00-05:00\n"
     ]
    }
   ],
   "source": [
    "dfs = {} # Store all the data for our tickers, even though for now its just SP500\n",
    "tickers = ['^GSPC',] # Ticker (identifier) for the SP500, which is arguably the most influential, we start with just this\n",
    "\n",
    "# Got this code from https://www.kaggle.com/code/xxxxyyyy80008/predict-stock-market-crashes \n",
    "for ticker in tickers:\n",
    "    cur_data = yf.Ticker(ticker)\n",
    "    hist = cur_data.history(period=\"max\")\n",
    "    print(ticker, cur_data.info['shortName'], ticker, hist.shape, hist.index.min(), hist.index.max())\n",
    "    dfs[ticker] = hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 15 # how large our window is\n",
    "TEST_SIZE = 0.2\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 20\n",
    "SHIFT_DAYS = 3 # how far ahead we want to predict market crashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Database creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates how much the price has gone down from its highest point, labeling those with a 0.005 quantile as crashes as per directive 1\n",
    "# NOTE: got help from chatgpt for this function to find .pct_change, .cummax and .quantile\n",
    "def calculate_drawdown_and_label(data, quantile_threshold=0.005):\n",
    "\n",
    "    data['daily_return'] = data['Close'].pct_change()\n",
    "\n",
    "    data['drawdown'] = data['Close'] / data['Close'].cummax() - 1\n",
    "\n",
    "    crash_threshold = data['drawdown'].quantile(quantile_threshold)\n",
    "\n",
    "    data['crash_label'] = (data['drawdown'] < crash_threshold).astype(int)\n",
    "\n",
    "    return data\n",
    "\n",
    "## Make the data we targetted the target (nice), we try to predict the market crashes 3 days in advance so shift_days = 3\n",
    "def prepare_target(data, shift_days=3):\n",
    "    data['target'] = (data['crash_label']).astype(int).shift(-shift_days) # complicated line that basically takes our data with crash label, turns the bools into an int, then shift the values by 3 days to predict in advance, since we want to predict 3 days in advance\n",
    "    return data\n",
    "\n",
    "# prep the data\n",
    "for ticker, data in dfs.items():\n",
    "    data = calculate_drawdown_and_label(data) # get the drawdown for our data\n",
    "    data = prepare_target(data, SHIFT_DAYS)\n",
    "    dfs[ticker] = data\n",
    "\n",
    "\n",
    "# Database done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, sequence_length=15): # data is our data we\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))  # Normalize our features between 0 and 1 to make sure its all the same, since prices and dropdown are very much not all on scale.\n",
    "    scaled_data = scaler.fit_transform(data[['Close', 'daily_return', 'drawdown']].dropna()) # our simple features we defined before + the ones from hist, scale them and create. .dropna removes the rows that have features which are nan i.e. rows with missing features\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(scaled_data) - sequence_length): # process data by sequences of days to learn. This is a window, so we go from one day to the next. for example, if we have day 1 to day 15 on iteration i, at iteration i + 1 we have day 2 to 16. this window is a single datapoint within a single minibatch.\n",
    "        days = scaled_data[i:i+sequence_length] # get the sequence of 15 days\n",
    "        X.append(days) # add to x the data as sequences of 15 days\n",
    "        y.append(data['target'].iloc[i + sequence_length]) # this is the target. It adds the value of the next sequence as the target, so we can predict future crashes\n",
    "\n",
    "    X = np.array(X) # turn into array for processing\n",
    "    y = np.array(y)\n",
    "     \n",
    "    \n",
    "    y = y[~np.isnan(y)]  # remove nan of the last few data points, since we shift by 3 days the last few points. i.e. for day 1, the target is day 4 for three days. so at the end of the seuquence wed get 3 empty datapoints. \n",
    "    X = X[:len(y)]  # cut those data points which had no y, i.e. the last few days\n",
    "    return X, y, scaler # return the scaler function to make sure when we process the test data and validation data, we can process the data before feeding it into our model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# now we process our data, here we do as if we had multiple tickers, in case we add more later\n",
    "sequence_length = SEQUENCE_LENGTH # hyperparameter for the length of the window. 15 is completely arbirtrary here, need to adjust to get better or worst results.\n",
    "X = {} # data of all the tickers\n",
    "y = {} # labels of all the tickers\n",
    "scalers = {} # scalers of all the tickers\n",
    "for ticker, data in dfs.items(): #iterate through each ticker, datapoint and the dataframes (dfs) we got from yf, and preprocess them then add them to the dictionnary\n",
    "    X[ticker], y[ticker], scalers[ticker] = preprocess_data(data, sequence_length=sequence_length) \n",
    "\n",
    "# Combine data from all our tickers, joining them on axis 0 i.e. we want to concatenate all the rows together\n",
    "X_all = np.concatenate([X[ticker] for ticker in X], axis=0)\n",
    "y_all = np.concatenate([y[ticker] for ticker in y], axis=0) # same thing for labels\n",
    "\n",
    "# give 20% as test size, 80% for our training data. this functino was found in the tutorial ipynb\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=TEST_SIZE, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adrienbelanger/Documents/GitHub/451-Project/.venv/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-stationary starting autoregressive parameters'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[1326 3502]\n",
      " [   2   23]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.27      0.43      4828\n",
      "         1.0       0.01      0.92      0.01        25\n",
      "\n",
      "    accuracy                           0.28      4853\n",
      "   macro avg       0.50      0.60      0.22      4853\n",
      "weighted avg       0.99      0.28      0.43      4853\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def forecast_arima(X_test, sequence_length, shift_days, threshold=0.005):\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "       # make sure we ge tonly the sequence length\n",
    "        if i + sequence_length > len(X_test):\n",
    "            break  \n",
    "        \n",
    "        train_sequence = X_test[i, :, 0]  \n",
    "\n",
    "        # 5 1 0 is arbitrary, need to test on mulyiple models\n",
    "        model = ARIMA(train_sequence, order=(5, 1, 0))  \n",
    "        model_fit = model.fit()\n",
    "\n",
    "        # clauclate the forecast on the next 3 days\n",
    "        forecast = model_fit.forecast(steps=shift_days)\n",
    "\n",
    "        # check the drawdown: NOTE: Got help\n",
    "        forecast_drawdown = (np.array(forecast) / max(train_sequence) - 1)\n",
    "        isit_crash = False\n",
    "        for dd in forecast_drawdown: # check if anything is lower than the treshold\n",
    "            \n",
    "            if (dd < -threshold):\n",
    "                isit_crash = True\n",
    "                break\n",
    "        \n",
    "        predictions.append(1 if isit_crash == 1 else 0) # make it a crash if its 0\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Generate predictions on test data\n",
    "predictions = forecast_arima(X_test, sequence_length=SEQUENCE_LENGTH, shift_days=SHIFT_DAYS)\n",
    "confusion_matrix_var = confusion_matrix(y_test[:len(predictions)], predictions)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting and Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics and final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [4867, 4853]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m##### GET ACCURACY AND CONFUSION MATRIX #####\u001b[39;00m\n\u001b[1;32m      2\u001b[0m predictions_as_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(arima_predictions[:\u001b[38;5;28mlen\u001b[39m(y_test)])\n\u001b[0;32m----> 3\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions_as_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m confusion_matrix_var \u001b[38;5;241m=\u001b[39m confusion_matrix(y_test, predictions_as_classes)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#############################################\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m### FINAL ACCURACY\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/451-Project/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/GitHub/451-Project/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:231\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    229\u001b[0m xp, _, device \u001b[38;5;241m=\u001b[39m get_namespace_and_device(y_true, y_pred, sample_weight)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/GitHub/451-Project/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:103\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[0;32m--> 103\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/451-Project/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    460\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [4867, 4853]"
     ]
    }
   ],
   "source": [
    "##### GET ACCURACY AND CONFUSION MATRIX #####\n",
    "predictions_as_classes = np.array(arima_predictions[:len(y_test)])\n",
    "accuracy = accuracy_score(y_test, predictions_as_classes)\n",
    "\n",
    "confusion_matrix_var = confusion_matrix(y_test, predictions_as_classes)\n",
    "\n",
    "#############################################\n",
    "\n",
    "\n",
    "### FINAL ACCURACY\n",
    "print(f\"Test Accuracy: {accuracy}\") # print our final accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m real_crashes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28msum\u001b[39m(y_test \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m# crashes in our test data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predicted_crashes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43m(\u001b[49m\u001b[43mpredictions_as_classes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m()) \u001b[38;5;66;03m# crashes are 1 in our predictions\u001b[39;00m\n\u001b[1;32m      3\u001b[0m predicted_correctly \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(confusion_matrix_var[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# tp in our confusion matrix\u001b[39;00m\n\u001b[1;32m      4\u001b[0m missed_crashes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(confusion_matrix_var[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# fn in our confusion matrix\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'sum'"
     ]
    }
   ],
   "source": [
    "real_crashes = int(sum(y_test == 1)) # crashes in our test data\n",
    "predicted_crashes = int((predictions_as_classes == 1).sum()) # crashes are 1 in our predictions\n",
    "predicted_correctly = int(confusion_matrix_var[1, 1])  # tp in our confusion matrix\n",
    "missed_crashes = int(confusion_matrix_var[1, 0])  # fn in our confusion matrix\n",
    "predicted_when_there_wasnt = int(confusion_matrix_var[0, 1])  # fp in our confusion matrix\n",
    "\n",
    "categories = [\n",
    "    \"Actual Crashes\",\n",
    "    \"Predicted Crashes\",\n",
    "    \"Correctly predicted Crashes\",\n",
    "    \"Missed Crashes\",\n",
    "    \"False alarms\"\n",
    "]\n",
    "values = [\n",
    "    real_crashes,\n",
    "    predicted_crashes,\n",
    "    predicted_correctly,\n",
    "    missed_crashes,\n",
    "    predicted_when_there_wasnt,\n",
    "]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(categories, values, color='orange')\n",
    "plt.title(\"Performance Analysis of our RNN\")\n",
    "plt.ylabel(\"Total crashes\")\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2.0, height, f'{height}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
