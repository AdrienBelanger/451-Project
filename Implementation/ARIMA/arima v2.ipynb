{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporary, for convenience\n",
    "\n",
    "DECISIONS:\n",
    "1. Calculate the drawdown and assign days with drawdown in the 99.5% quantile as crash (i.e. label=1). \n",
    "2. The target is shift 3 days to make the problem as \"Predict if there will be a market crash 3 days later\".\n",
    "\n",
    "\n",
    "\n",
    "To-do: \n",
    "- More Plots?\n",
    "- Add data\n",
    "- more hyperparameter search\n",
    "- better accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and imports   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yfinance as yf # to get our data\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler # to normalize our classes\n",
    "from sklearn.model_selection import train_test_split # from tutorial code\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix # needed for plotting\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Data from Yahoo Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^GSPC S&P 500 ^GSPC (24348, 7) 1927-12-30 00:00:00-05:00 2024-12-03 00:00:00-05:00\n"
     ]
    }
   ],
   "source": [
    "dfs = {} # Store all the data for our tickers, even though for now its just SP500\n",
    "tickers = ['^GSPC',] # Ticker (identifier) for the SP500, which is arguably the most influential, we start with just this\n",
    "\n",
    "# Got this code from https://www.kaggle.com/code/xxxxyyyy80008/predict-stock-market-crashes \n",
    "for ticker in tickers:\n",
    "    cur_data = yf.Ticker(ticker)\n",
    "    hist = cur_data.history(period=\"max\")\n",
    "    print(ticker, cur_data.info['shortName'], ticker, hist.shape, hist.index.min(), hist.index.max())\n",
    "    dfs[ticker] = hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 15 # how large our window is\n",
    "TEST_SIZE = 0.2\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 20\n",
    "SHIFT_DAYS = 3 # how far ahead we want to predict market crashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Database creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates how much the price has gone down from its highest point, labeling those with a 0.005 quantile as crashes as per directive 1\n",
    "# NOTE: got help from chatgpt for this function to find .pct_change, .cummax and .quantile\n",
    "def calculate_drawdown_and_label(data, quantile_threshold=0.005):\n",
    "\n",
    "    data['daily_return'] = data['Close'].pct_change()\n",
    "\n",
    "    data['drawdown'] = data['Close'] / data['Close'].cummax() - 1\n",
    "\n",
    "    crash_threshold = data['drawdown'].quantile(quantile_threshold)\n",
    "\n",
    "    data['crash_label'] = (data['drawdown'] < crash_threshold).astype(int)\n",
    "\n",
    "    return data\n",
    "\n",
    "## Make the data we targetted the target (nice), we try to predict the market crashes 3 days in advance so shift_days = 3\n",
    "def prepare_target(data, shift_days=3):\n",
    "    data['target'] = (data['crash_label']).astype(int).shift(-shift_days) # complicated line that basically takes our data with crash label, turns the bools into an int, then shift the values by 3 days to predict in advance, since we want to predict 3 days in advance\n",
    "    return data\n",
    "\n",
    "# prep the data\n",
    "for ticker, data in dfs.items():\n",
    "    data = calculate_drawdown_and_label(data) # get the drawdown for our data\n",
    "    data = prepare_target(data, SHIFT_DAYS)\n",
    "    dfs[ticker] = data\n",
    "\n",
    "\n",
    "# Database done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, sequence_length=15): # data is our data we\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))  # Normalize our features between 0 and 1 to make sure its all the same, since prices and dropdown are very much not all on scale.\n",
    "    scaled_data = scaler.fit_transform(data[['Close', 'daily_return', 'drawdown']].dropna()) # our simple features we defined before + the ones from hist, scale them and create. .dropna removes the rows that have features which are nan i.e. rows with missing features\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(scaled_data) - sequence_length): # process data by sequences of days to learn. This is a window, so we go from one day to the next. for example, if we have day 1 to day 15 on iteration i, at iteration i + 1 we have day 2 to 16. this window is a single datapoint within a single minibatch.\n",
    "        days = scaled_data[i:i+sequence_length] # get the sequence of 15 days\n",
    "        X.append(days) # add to x the data as sequences of 15 days\n",
    "        y.append(data['target'].iloc[i + sequence_length]) # this is the target. It adds the value of the next sequence as the target, so we can predict future crashes\n",
    "\n",
    "    X = np.array(X) # turn into array for processing\n",
    "    y = np.array(y)\n",
    "     \n",
    "    \n",
    "    y = y[~np.isnan(y)]  # remove nan of the last few data points, since we shift by 3 days the last few points. i.e. for day 1, the target is day 4 for three days. so at the end of the seuquence wed get 3 empty datapoints. \n",
    "    X = X[:len(y)]  # cut those data points which had no y, i.e. the last few days\n",
    "    return X, y, scaler # return the scaler function to make sure when we process the test data and validation data, we can process the data before feeding it into our model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# now we process our data, here we do as if we had multiple tickers, in case we add more later\n",
    "sequence_length = SEQUENCE_LENGTH # hyperparameter for the length of the window. 15 is completely arbirtrary here, need to adjust to get better or worst results.\n",
    "X = {} # data of all the tickers\n",
    "y = {} # labels of all the tickers\n",
    "scalers = {} # scalers of all the tickers\n",
    "for ticker, data in dfs.items(): #iterate through each ticker, datapoint and the dataframes (dfs) we got from yf, and preprocess them then add them to the dictionnary\n",
    "    X[ticker], y[ticker], scalers[ticker] = preprocess_data(data, sequence_length=sequence_length) \n",
    "\n",
    "# Combine data from all our tickers, joining them on axis 0 i.e. we want to concatenate all the rows together\n",
    "X_all = np.concatenate([X[ticker] for ticker in X], axis=0)\n",
    "y_all = np.concatenate([y[ticker] for ticker in y], axis=0) # same thing for labels\n",
    "\n",
    "# give 20% as test size, 80% for our training data. this functino was found in the tutorial ipynb\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=TEST_SIZE, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use ARIMA for short forecast over 3 days, then binary classification depending on result\n",
    "\n",
    "## Use the sliding window to predict the next three days. If that prediction includes a crash as per our definition, then return 1, else 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting and Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics and final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### GET ACCURACY AND CONFUSION MATRIX #####\n",
    "predictions_as_probabilities = None #rnn.predict(X_test) NEED TO CHANGE\n",
    "predictions_as_classes = None #(predictions_as_probabilities > 0.5).astype(int) # make our predictions binary, a simple scaler that puts values higher than 0.5 to 1 and lower 0. astypeint to return 0 and 1 instead of true false array.\n",
    "accuracy = None #accuracy_score(y_test, predictions_as_classes) # calculate our accuracy\n",
    "\n",
    "confusion_matrix_var = None #confusion_matrix(y_test, predictions_as_classes) # plot the confusion matrix\n",
    "#############################################\n",
    "\n",
    "\n",
    "### FINAL ACCURACY\n",
    "#print(f\"Test Accuracy: {accuracy}\") # print our final accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m real_crashes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28msum\u001b[39m(y_test \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m# crashes in our test data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predicted_crashes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43m(\u001b[49m\u001b[43mpredictions_as_classes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m()) \u001b[38;5;66;03m# crashes are 1 in our predictions\u001b[39;00m\n\u001b[1;32m      3\u001b[0m predicted_correctly \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(confusion_matrix_var[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# tp in our confusion matrix\u001b[39;00m\n\u001b[1;32m      4\u001b[0m missed_crashes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(confusion_matrix_var[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# fn in our confusion matrix\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'sum'"
     ]
    }
   ],
   "source": [
    "real_crashes = int(sum(y_test == 1)) # crashes in our test data\n",
    "predicted_crashes = int((predictions_as_classes == 1).sum()) # crashes are 1 in our predictions\n",
    "predicted_correctly = int(confusion_matrix_var[1, 1])  # tp in our confusion matrix\n",
    "missed_crashes = int(confusion_matrix_var[1, 0])  # fn in our confusion matrix\n",
    "predicted_when_there_wasnt = int(confusion_matrix_var[0, 1])  # fp in our confusion matrix\n",
    "\n",
    "categories = [\n",
    "    \"Actual Crashes\",\n",
    "    \"Predicted Crashes\",\n",
    "    \"Correctly predicted Crashes\",\n",
    "    \"Missed Crashes\",\n",
    "    \"False alarms\"\n",
    "]\n",
    "values = [\n",
    "    real_crashes,\n",
    "    predicted_crashes,\n",
    "    predicted_correctly,\n",
    "    missed_crashes,\n",
    "    predicted_when_there_wasnt,\n",
    "]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(categories, values, color='orange')\n",
    "plt.title(\"Performance Analysis of our RNN\")\n",
    "plt.ylabel(\"Total crashes\")\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2.0, height, f'{height}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
