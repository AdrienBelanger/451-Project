\documentclass[12pt, letterpaper]{article}
\usepackage[margin=1in, top=0.6in]{geometry}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{titling}
\usepackage{enumitem}
\usepackage{ragged2e}
\usepackage{comment}
\usepackage{xcolor}
\usepackage[
backend=biber,
style=ieee,
sorting=none,
]{biblatex}
\addbibresource{final_bibliography.bib}

\title{
    Empirical Review of Models used for Predicting Financial Market Crashes Using Market Data 
    \vspace{1em} \\
    {\large Project: Final Report}
}
\author{\normalsize Ping-Chieh Tu \\ Adrien Bélanger \\ Inigo Torres}
\date{\vspace{2em} November 22$^{\text{nd}}$ 2024}

\begin{document}

\begin{titlepage}
    \centering
    \vspace*{2in}
    {\LARGE \textbf{Empirical Review of Models used for Predicting Financial Market Crashes Using Market Data}}\\
    \vspace*{0.5in}
    {\large COMP 451 Final Project: Final Report}\\[4in]
    \normalsize
    By Ping-Chieh Tu, Adrien Bélanger, Inigo Torres \\ [3em]
    December 13$^{\text{th}}$ 2024
\end{titlepage}
\pagebreak

\subsection*{Introduction \textcolor{red}{Inigo}}

The volatility and complexity of financial markets have always been a significant challenge for the management of modern economies. 
For instance, the abrupt declines in the markets, often known in finance as crashes, can lead to widespread financial losses, economic recessions, and a loss of confidence in the stability of financial systems. 

The idea of being able to anticipate and predict such market fluctuations is not new and has been widely studied from a mathematical point of view. 
However, conventional methods used by econometricians for time series analysis, such as Linear Trend Projection or Weighted Moving Average, may be effective for markets with general stationary trends but lack effectiveness for highly volatile ones. 
Modern machine learning techniques, particularly those involving neural networks and attention-based architectures, may provide a new pathway to surpassing these traditional techniques.

In this project, we propose to make an extensive comparative study of the effectiveness of 3 of the most widely used methods for Time series forecasting today: Recurrent Neural Networks (RNNs), Transformer-based architectures, and the Autoregressive Integrated Moving Average (ARIMA) model. 

Each of these models is developed with a distinct methodological perspective. 
ARIMA models, based on classic statistics and linear algebra concepts, have long been a go-to tool for time series forecasting. 
RNNs, specifically Long Short-Term Memory (LSTM) networks, offer a non-linear and data-driven alternative that can capture long-range dependencies while reducing issues of vanishing or exploiting gradients. 
Transformers, which introduce attention mechanisms, aim to further improve predictive capabilities by focusing selectively on critical segments of past information, often achieving state-of-the-art performance in various sequential prediction tasks.

Our main objective is then to develop each of these models and subject them to a series of tests using real historical financial data to determine their effectiveness in predicting financial crashes. 

Although the world of finance is changing, and there are no specific parameters since each financial market is governed by its own rules, we will test each of these models under the same conditions and thus establish an empirical review to provide clear insights into the strengths and limitations of each methodology. We aim to guide researchers and practitioners in selecting suitable models for short-term market crash prediction, ultimately contributing to more robust risk management strategies.



\subsection*{Litterature Review \textcolor{cyan}{Adrien}}
% (Where does our project fit in context of  other articles)
    We are not the first to attempt to compare ML models performance on their prediction of market crashes using SP500 historical data. Multiple approaches have been tried and tested. Time series analysis has used ARIMA and RNN based models, with the more recent addition of Transformers boosting and improving their performance  \cite{Okpeke, Ahmed, ArunKumar}. Reviews of these models have been done before, but the comparison of these three models on short-term prediction using very recent market data is lacking in litterature. 
    \paragraph*{}
    % COMPAre OTHER ARTICLES that did this
    One article compared Linear Regression and Autoregressive Moving Average (ARIMA) to predict the volatility and trend of SP500. \cite{sp500arimalstmregression}. 
    Key-findings show that ARIMA struggled on short term predictions, particularly during the 1930s and 2020 volatile markets. 
    
    Zhou et al. demonstrated that transformers handle long-term dependencies well with the "Informer model," which efficiently handles long sequences and improves trend forecasting accuracy using a "self-attention" mechanism \cite{zhou2021informer}. Moreover, Lim et al. were even able to outperform traditional methods (like ARIMA) by combining the Transformer architecture with recurrent layers \cite{lim2021temporal}. 
    
    Zhen Zeng studied the applicability of Transformers combined with RNNs in trend detection specifically for financial time series in his paper \cite{zeng2023financial}. He concluded that combining these two architectures significantly improved forecasting accuracy for intraday stock price movements of the SP500.
    \paragraph*{}
    % Explore definitions of market crashes and which data was used
    Market Crashes do not hold a single definition. While historical data tags specific periods as depressions and bubbles, there are no specific metrics that are universally defined. Some have defined it as a rapid decline of 20\% or more from a recent peak over a short period \cite{Fonville, Investo}. Others still have defined them as a drawdown of 99.5\% quantile \cite{99.5quantile}.
    \paragraph*{}
    % Explore models implementation
    %%% RNN
    
    Time series in the stock market often exhibit complex trends and non-linear patterns. 
    Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNN) are designed to learn long-term trends in sequential data, making them very good at modeling and forecasting financial market time series \cite{hochreiter1997, greff2017}.

    LSTM networks address the vanishing gradient problem encountered in standard RNNs on this type of data by introducing specialized LSTM cells. These cells incorporate gates to control flow, allowing LSTMs to forget and remember information over long sequences \cite{hochreiter1997}.

    The LSTM cell can be defined as follows \cite{hochreiter1997}:

\begin{align*} 
    i_t &= \sigma(W_i x_t + U_i h_{t-1} + b_i), \\ f_t &= \sigma(W_f x_t + U_f h_{t-1} + b_f), \\ o_t &= \sigma(W_o x_t + U_o h_{t-1} + b_o), \\ \tilde{c}t &= \tanh(W_c x_t + U_c h{t-1} + b_c), \\ c_t &= f_t \cdot c_{t-1} + i_t \cdot \tilde{c}_t, \\ h_t &= o_t \cdot \tanh(c_t), 
\end{align*}

where

\begin{enumerate}[label=-]
    \item $x_t$ is the input at time $t$, representing market data (e.g., stock price, volume),
    \item $h_{t-1}$ is the hidden state (output) of the previous time step,
    \item $i_t$, $f_t$, $o_t$ are the input gate, forget gate, and output gate
    \item $\tilde{c}_t$ is the candidate memory cell state,
    \item $c_t$ is the cell state at time $t$, capturing long-term dependencies,
    \item $W$ and $U$ are the weight matrices and b is the bias vector,
    \item $\sigma$ is the sigmoid activation function
\end{enumerate}

The \textit{forget} gate, $f_t$, determines how much of the previous cell $c_{t-1}$ is retained. The \textit{input} gate $i_t$ updates the cell with new information. The \textit{output} gate $o_t$ controls the hidden state, which is the LSTM output at time $t$ \cite{hochreiter1997}.


%%%%%%%%%%%%%%%%%%%%%%%%
    \
    
    %%% ARIMA
    Time series in the stock market are non-stationary, which means that their statistical properties (Avg, median) change over time. ARIMA models can be adapted to capture the behavior of non-stationary time-series \cite{ho2021}. A mehodology formalized by Box et Al. in 2015 to apply the ARIMA model with a moving average component \cite{box2015}.
    This makes ARIMA suitable for predicting non-stationary time series, such as financial markets, as demonstrated by M. K. Ho et al. in their paper \cite{ho2021}.
    The ARIMA model is defined mathematically as \cite{box2015}:
    {\small
    \begin{align*}
        \varphi (B) z_t = \phi (B)\nabla^d z_t = \theta_0 + \theta (B)a_t
    \end{align*}
    where
    \begin{enumerate}[label=-]
        \item $\phi(B)$ is the autoregressive part with $p$ degrees,
        \item $\theta (B)$ is the moving average part with $q$ degrees,
        \item $\nabla$ is the integrated (degree of differencing) part with $d$ degrees,
        \item and $\theta_0$ is the constant term.
    \end{enumerate}
    }

    The use of ACF (autocorrelation function) and PACF (partial ACF) will help us decide the hyperparameters $p, d, q$ for building our model \cite{hyndman2018}, and performing Grid-Search on the specific hyperparameters will help us pin point the exact best accuracy we can get.
    
    \

    %%% TRANSFORMERS
    Transformers are based on self-attention mechanisms. They are able to weigh the importance of input elements by computing \textit{attention scores}.
    While transformers were originally developed for natural language processing tasks, they have become popular for time-series forecasting due to their ability to handle long-range dependencies \cite{vaswani2017attention}.



    Unlike RNNs, Transformers process all steps at the same time, enabling parallel computation and improved efficiency for long-range dependency learning \cite{vaswani2017attention}.

The core attention scoring by scaled dotv product is defined as \cite{vaswani2017attention}:
{\small \begin{align*} \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V, \end{align*} }
where:
\begin{enumerate}[label=-] \item $Q$ (queries), $K$ (keys), and $V$ (values) are linear projections of the input data,
\item  is the dimension of the key vectors,
\item The softmax function ensures that attention scores are normalized.
\end{enumerate}

Transformers eliminate the need for recurrent connections by relying entirely on attention mechanisms. This allows them model long-range dependencies more effectively than RNNs, which sometimes struggle with sequential bottlenecks and vanishing gradients \cite{vaswani2017attention}.

While originally developed for natural language processing tasks, Transformers have been increasingly applied to time-series forecasting \cite{zeng2023financial}.
Given the volatility and complexity of financial markets, their ability to capture complex patterns makes them an ideal choice for financial market forecasting \cite{wu2020deep}.

\paragraph*{}
% Explore analysis criteria 
    Drastic market crashes are rare [cite]. Models can achieve extremely high accuracy by simply predicting no market crash for every datapoint. Other methods are thus needed to evaluate the models. Others in litterature have used many methods, such as evaluating true positives and true negatives [cite]. Other have used mean absolute error (MAE) to assess prediction accuracy \cite{hyndman2018}. Finally, some have used runtime and resource usage for practical feasibility to assess their performance \cite{lim2021temporal}.

\subsection*{Methodology \textcolor{cyan}{Adrien for market crash and Experiment choice} \textcolor{violet}{Everyone for their assigned model}}
% (Our models and market crash definition, how they work, why we chose them, the experiments we chose and why we chose them) + HYPOTHESIS on each experiment
\subsubsection*{Definitions used in our project}


\subsubsection*{Database and preprocessing}

\subsubsection*{RNN Implementation}

Our RNN code was based on the \textit{Keras} Sequential model, using the pre-made LSTM layers \cite{keras2024lstm,keras2024sequential}. This was used since our project aimed to be a comparison, and not one of implementation. The c

\subsubsection*{ARIMA Implementation}

\subsubsection*{Transformers Implementation}

\subsubsection*{Experiments}
    \begin{enumerate}
        \item Adrien - 30 days sliding window, 14 days shift days
        \item Oscar - 14 days sliding window, 7 days shift days
        \item Inigo - 7 days sliding window, 3 days shift days
    \end{enumerate}
\subsection*{Empirical Evaluation \textcolor{green}{Oscar}}
% (plots, statements of comparison (without committing to a full-depth comparison, thats in discussion) of all the experiments)
\subsection*{Discussion \textcolor{violet}{Everyone discusses their experiment}}
% (discussing the results and comparing them and conclude on our hypothesis)
\subsection*{Conclusion \textcolor{violet}{Everyone does the conclusion for their own experiments}}
%  (Our conclusion on our results, so how each model fared, how fast they trained, how efficiently, which model we think is best in each context, etc)
\subsection*{Future Directions \textcolor{red}{Inigo}}
% (where we could take this project next, how we could improve it etc)
\subsection*{Self-Assessment \textcolor{green}{Oscar}}
% (how we feel about the project, what would we have done differently etc.)
\subsection*{Contributions \textcolor{violet}{Everyone writes their own contribution}}
% contributions: contributions of each member to the report. If there is a disagreement, you can inlcude a longer contribution section in the appendix where each member explains their version.
\begin{enumerate}
    \item Adrien - 
        For the report, my main contributions were for writing the Litterature Review and parts of the Methodology, Discussion and Conclusion. I've also contributed to the code by implementing the RNN model, consolidating the three models in a notebook and helping design the experiments.
    \item Ping-Chieh
    \item Inigo
\end{enumerate}

\pagebreak
\subsection*{Appendix}
\printbibliography
\end{document}
