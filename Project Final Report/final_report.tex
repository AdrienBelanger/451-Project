\documentclass[12pt, letterpaper]{article}
\usepackage[margin=1in, top=0.6in]{geometry}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{titling}
\usepackage{enumitem}
\usepackage{ragged2e}
\usepackage{comment}
\usepackage{xcolor}
\usepackage[
backend=biber,
style=ieee,
sorting=none,
]{biblatex}
\addbibresource{final_bibliography.bib}

\title{
    Empirical Review of Models used for Predicting Financial Market Crashes Using Market Data 
    \vspace{1em} \\
    {\large Project: Final Report}
}
\author{\normalsize Ping-Chieh Tu \\ Adrien Bélanger \\ Inigo Torres}
\date{\vspace{2em} November 22$^{\text{nd}}$ 2024}

\begin{document}

\begin{titlepage}
    \centering
    \vspace*{2in}
    {\LARGE \textbf{Empirical Review of Models used for Predicting Financial Market Crashes Using Market Data}}\\
    \vspace*{0.5in}
    {\large COMP 451 Final Project: Final Report}\\[4in]
    \normalsize
    By Ping-Chieh Tu, Adrien Bélanger, Inigo Torres \\ [3em]
    December 13$^{\text{th}}$ 2024
\end{titlepage}
\pagebreak

\subsection*{Introduction \textcolor{red}{Inigo}}

**thesis statement**

% Project in context, give background on what is a market crash, etc Inigo (?)


\subsection*{Litterature Review \textcolor{cyan}{Adrien}}
% (Where does our project fit in context of  other articles)
    We are not the first to attempt to compare ML models performance on their prediction of market crashes using SP500 historical data. Multiple approaches have been tried and tested. Time series analysis has used ARIMA and RNN based models, with the more recent addition of Transformers boosting and improving their performance  \cite{Okpeke, Ahmed, ArunKumar}. Reviews of these models have been done before, but the comparison of these three models on short-term prediction using very recent market data is lacking in litterature. 
    \paragraph*{}
    % COMPAre OTHER ARTICLES that did this
    One article compared Linear Regression and Autoregressive Moving Average (ARIMA) to predict the volatility and trend of SP500. \cite{sp500arimalstmregression}. 
    Key-findings show that ARIMA struggled on short term predictions, particularly during the 1930s and 2020 volatile markets. 
    
    Zhou et al. demonstrated that transformers handle long-term dependencies well with the "Informer model," which efficiently handles long sequences and improves trend forecasting accuracy using a "self-attention" mechanism \cite{zhou2021informer}. Moreover, Lim et al. were even able to outperform traditional methods (like ARIMA) by combining the Transformer architecture with recurrent layers \cite{lim2021temporal}. 
    
    Zhen Zeng studied the applicability of Transformers combined with RNNs in trend detection specifically for financial time series in his paper \cite{zeng2023financial}. He concluded that combining these two architectures significantly improved forecasting accuracy for intraday stock price movements of the SP500.
    \paragraph*{}
    % Explore definitions of market crashes and which data was used
    Market Crashes do not hold a single definition. While historical data tags specific periods as depressions and bubbles, there are no specific metrics that are universally defined \cite{Fonville, Investo}. 
    \paragraph*{}
    % Explore models implementation
    %%% RNN
    

    Time series in the stock market often exhibit complex trends and non-linear patterns. 
    Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNN) are designed to learn long-term trends in sequential data, making them very good at modeling and forecasting financial market time series \cite{hochreiter1997, greff2017}.

    LSTM networks address the vanishing gradient problem encountered in standard RNNs on this type of data by introducing specialized LSTM cells. These cells incorporate gates to control flow, allowing LSTMs to forget and remember information over long sequences \cite{hochreiter1997}.


    The LSTM cell can be defined as follows \cite{hochreiter1997}:

\begin{align*} 
    i_t &= \sigma(W_i x_t + U_i h_{t-1} + b_i), \\ f_t &= \sigma(W_f x_t + U_f h_{t-1} + b_f), \\ o_t &= \sigma(W_o x_t + U_o h_{t-1} + b_o), \\ \tilde{c}t &= \tanh(W_c x_t + U_c h{t-1} + b_c), \\ c_t &= f_t \cdot c_{t-1} + i_t \cdot \tilde{c}_t, \\ h_t &= o_t \cdot \tanh(c_t), 
\end{align*}

where:

\begin{itemize}
    \item $x_t$ is the input at time $t$, representing market data (e.g., stock price, volume),
    \item $h_{t-1}$ is the hidden state (output) of the previous time step,
    \item $i_t$, $f_t$, $o_t$ are the input gate, forget gate, and output gate
    \item $\tilde{c}_t$ is the candidate memory cell state,
    \item $c_t$ is the cell state at time $t$, capturing long-term dependencies,
    \item $W$ and $U$ are the weight matrices and b is the bias vector,
    \item $\sigma$ is the sigmoid activation function
\end{itemize}

The \textit{forget} gate, $f_t$, determines how much of the previous cell $c_{t-1}$ is retained. The \textit{input} gate $i_t$ updates the cell with new information. The \textit{output} gate $o_t$ controls the hidden state, which is the LSTM output at time $t$ \cite{hochreiter1997}.


%%%%%%%%%%%%%%%%%%%%%%%%
    \
    
    %%% ARIMA
    Time series in the stock market are non-stationary, which means that their statistical properties (Avg, median) change over time. ARIMA models can be adapted to capture the behavior of non-stationary time-series \cite{ho2021}. A mehodology formalized by Box et Al. in 2015 to apply the ARIMA model with a moving average component \cite{box2015}.
    This makes ARIMA suitable for predicting non-stationary time series, such as financial markets, as demonstrated by M. K. Ho et al. in their paper \cite{ho2021}.
    The ARIMA model is defined mathematically as \cite{box2015}:
    {\small
    \begin{align*}
        \varphi (B) z_t = \phi (B)\nabla^d z_t = \theta_0 + \theta (B)a_t
    \end{align*}
    where
    \begin{enumerate}[label=\arabic*.]
        \item $\phi(B)$ is the autoregressive part with $p$ degrees,
        \item $\theta (B)$ is the moving average part with $q$ degrees,
        \item $\nabla$ is the integrated (degree of differencing) part with $d$ degrees,
        \item and $\theta_0$ is the constant term.
    \end{enumerate}
    }

    The use of ACF (autocorrelation function) and PACF (partial ACF) will help us decide the hyperparameters $p, d, q$ for building our model \cite{hyndman2018}, and performing Grid-Search on the specific hyperparameters will help us pin point the exact best accuracy we can get.
    
    \

    %%% TRANSFORMERS
    Transformers are based on self-attention mechanisms. They are able to weigh the importance of input elements by computing \textit{attention scores}. While transformers were originally developed for natural language processing tasks, they have become popular for time-series forecasting due to their ability to handle long-range dependencies \cite{vaswani2017attention}.

    This makes transformers an ideal choice for predicting trends in volatile time series, as they can capture complex temporal patterns by assigning varying importance to different time steps. 

    Their adaptable mathematical design makes them a powerful tool for predicting any kind of time series. Financial markets are no exception. Hence, the literature suggests that the self-attention mechanism of Transformers makes them a pertinent choice for financial trend forecasting.



    \paragraph*{}
    % Explore analysis criteria 
    Drastic market crashes are rare [cite]. Models can achieve extremely high accuracy by simply predicting no market crash for every datapoint. Other methods are thus needed to evaluate the models. Others in litterature have used many methods, such as evaluating true positives and true negatives [cite]. Other have used mean absolute error (MAE) to assess prediction accuracy \cite{hyndman2018}. Finally, some have used runtime and resource usage for practical feasibility to assess their performance \cite{lim2021temporal}.

\subsection*{Methodology \textcolor{cyan}{Adrien for market crash and Experiment choice} \textcolor{violet}{Everyone for their assigned model}}
% (Our models and market crash definition, how they work, why we chose them, the experiments we chose and why we chose them) + HYPOTHESIS on each experiment
\subsubsection*{Definitions used in our project}



\subsubsection*{RNN Implementation}

RNN

\subsubsection*{ARIMA Implementation}

\subsubsection*{Transformers Implementation}

\subsubsection*{Experiments}
    \begin{enumerate}
        \item Adrien - 30 days sliding window, 14 days shift days
        \item Oscar - 14 days sliding window, 7 days shift days
        \item Inigo - 7 days sliding window, 3 days shift days
    \end{enumerate}
\subsection*{Empirical Evaluation \textcolor{green}{Oscar}}
% (plots, statements of comparison (without committing to a full-depth comparison, thats in discussion) of all the experiments)
\subsection*{Discussion \textcolor{violet}{Everyone discusses their experiment}}
% (discussing the results and comparing them and conclude on our hypothesis)
\subsection*{Conclusion \textcolor{violet}{Everyone does the conclusion for their own experiments}}
%  (Our conclusion on our results, so how each model fared, how fast they trained, how efficiently, which model we think is best in each context, etc)
\subsection*{Future Directions \textcolor{red}{Inigo}}
% (where we could take this project next, how we could improve it etc)
\subsection*{Self-Assessment \textcolor{green}{Oscar}}
% (how we feel about the project, what would we have done differently etc.)
\subsection*{Contributions \textcolor{violet}{Everyone writes their own contribution}}
% contributions: contributions of each member to the report. If there is a disagreement, you can inlcude a longer contribution section in the appendix where each member explains their version.
\begin{enumerate}
    \item Adrien - 
        For the report, my main contributions were for writing the Litterature Review and parts of the Methodology, Discussion and Conclusion. I've also contributed to the code by implementing the RNN model, consolidating the three models in a notebook and helping design the experiments.
    \item Ping-Chieh
    \item Inigo
\end{enumerate}

\pagebreak
\subsection*{Appendix}
\printbibliography
\end{document}
