@article{Okpeke,
   author = {Okpeke, Patience and Paul, Patience Okpeke and Iyelolu, Toluwalase Vanessa},
   title = {Predicting stock market crashes with machine learning: A review and methodological proposal},
   journal = {Open Access Research Journal of Science and Technology},
   year = {2024},
   type = {Journal Article}
}

@article{Ahmed,
   author = {Ahmed, Sabeen and Nielsen, Ian E. and Tripathi, Aakash and Siddiqui, Shamoon and Ramachandran, Ravi P. and Rasool, Ghulam},
   title = {Transformers in Time-Series Analysis: A Tutorial},
   journal = {Circuits Syst. Signal Process.},
   volume = {42},
   number = {12},
   pages = {7433–7466},
   keywords = {Positional encoding, Self-attention, Time-series, Transformer},
   ISSN = {0278-081X},
   DOI = {10.1007/s00034-023-02454-8},
   url = {https://doi.org/10.1007/s00034-023-02454-8},
   year = {2023},
   type = {Journal Article}
}

@article{ArunKumar,
   author = {ArunKumar, K. E. and Kalaga, Dinesh V. and Mohan Sai Kumar, Ch and Kawaji, Masahiro and Brenza, Timothy M.},
   title = {Comparative analysis of Gated Recurrent Units (GRU), long Short-Term memory (LSTM) cells, autoregressive Integrated moving average (ARIMA), seasonal autoregressive Integrated moving average (SARIMA) for forecasting COVID-19 trends},
   journal = {Alexandria Engineering Journal},
   volume = {61},
   number = {10},
   pages = {7585-7603},
   abstract = {Several machine learning and deep learning models were reported in the literature to forecast COVID-19 but there is no comprehensive report on the comparison between statistical models and deep learning models. The present work reports a comparative time-series analysis of deep learning techniques (Recurrent Neural Networks with GRU and LSTM cells) and statistical techniques (ARIMA and SARIMA) to forecast the country-wise cumulative confirmed, recovered, and deaths. The Gated Recurrent Units (GRU), Long Short-Term Memory (LSTM) cells based on Recurrent Neural Networks (RNN), ARIMA and SARIMA models were trained, tested, and optimized to forecast the trends of the COVID-19. We deployed python to optimize the parameters of ARIMA which include (p, d, q) representing autoregressive and moving average terms and parameters of SARIMA model include additional seasonal terms which are denoted by (P, D, Q). Similarly, for LSTM and GRU based RNN models’ parameters (number of layers, hidden size, learning rate and number of epochs) are optimized by deploying PyTorch machine learning framework. The best model was chosen based on the lowest Mean Square Error (MSE) and Root Mean Squared Error (RMSE) values. For most of the time-series data of the countries, deep learning-based models LSTM and GRU outperformed statistical ARIMA and SARIMA models, with an RMSE values that are 40 folds less than that of the ARIMA models. But for some countries statistical (ARIMA, SARIMA) models outperformed deep learning models. Further, we emphasize the importance of various factors such as age, preventive measures and healthcare facilities etc. that play vital role on the rapid spread of COVID-19 pandemic.},
   keywords = {COVID-19 pandemic
Gated Recurrent Units (GRUs)
Long Short-Term Memory (LSTM) cells
Recurrent Neural Networks (RNNs)
Auto Regressive Integrated Moving Average (ARIMA)
Seasonal Auto Regressive Integrated Moving Average (SARIMA)},
   ISSN = {1110-0168},
   DOI = {https://doi.org/10.1016/j.aej.2022.01.011},
   url = {https://www.sciencedirect.com/science/article/pii/S1110016822000138},
   year = {2022},
   type = {Journal Article}
}

@article{Chhajer,
   author = {Chhajer, Parshv and Shah, Manan and Kshirsagar, Ameya},
   title = {The applications of artificial neural networks, support vector machines, and long–short term memory for stock market prediction},
   journal = {Decision Analytics Journal},
   volume = {2},
   pages = {100015},
   abstract = {The future is unknown and uncertain, but there are ways to predict future events and reap the rewards safely. One such opportunity is the application of machine learning and artificial intelligence for stock market prediction. The stock market is turbulent, yet using artificial intelligence to make calculated predictions is possible and advisable before investing. This study presents an overview of artificial intelligence and machine learning as predictive analytics tools in the stock market. We discuss the strengths and weaknesses of machine learning for stock market prediction and provide some insight into the opportunities and threats in applying advanced technologies for stock market prediction. We further study the applications of three machine learning technologies in the stock market prediction, including artificial neural networks, support vector machines, and long–short term memory.},
   keywords = {Artificial Neural Network
Support Vector Machines
Long short-term memory
Stock Forecasting
Predictive analytics},
   ISSN = {2772-6622},
   DOI = {https://doi.org/10.1016/j.dajour.2021.100015},
   url = {https://www.sciencedirect.com/science/article/pii/S2772662221000102},
   year = {2022},
   type = {Journal Article}
}

@article{Schmidt,
   author = {Schmidt, Robin M.},
   title = {Recurrent Neural Networks (RNNs): A gentle Introduction and Overview},
   journal = {CoRR},
   volume = {abs/1912.05911},
   url = {http://arxiv.org/abs/1912.05911},
   year = {2019},
   type = {Journal Article}
}

@techreport{Mishkin,
 title = "U.S. Stock Market Crashes and Their Aftermath: Implications for Monetary Policy",
 author = "Mishkin, Frederic S and White, Eugene N",
 institution = "National Bureau of Economic Research",
 type = "Working Paper",
 series = "Working Paper Series",
 number = "8992",
 year = "2002",
 month = 6,
 doi = {10.3386/w8992},
 URL = "http://www.nber.org/papers/w8992",
 abstract = {This paper examines fifteen historical episodes of stock market crashes and their aftermath in the United States over the last one hundred years. Our basic conclusion from studying these episodes is that financial instability is the key problem facing monetary policy makers and not stock market crashes, even if they reflect the possible bursting of a bubble. With a focus on financial stability rather than the stock market, the response of central banks to stock market fluctuations is more likely to be optimal and maintain support for the independence of the central bank.},
}

@article{ho2021, 
  author    = {M. K. Ho and others},
  title     = {Application of ARIMA Models for Non-Stationary Time Series},
  journal   = {Journal of Physics: Conference Series},
  volume    = {1988},
  number    = {1},
  pages     = {012041},
  year      = {2021},
  doi       = {10.1088/1742-6596/1988/1/012041},
  url       = {https://iopscience.iop.org/article/10.1088/1742-6596/1988/1/012041/pdf},
  note      = {See also Box et al. [2015] and Hyndman et al. [2018]}
}

@book{box2015,
  author    = {George E. P. Box and Gwilym M. Jenkins and Gregory C. Reinsel and Greta M. Ljung},
  title     = {Time Series Analysis: Forecasting and Control},
  edition   = {5th},
  publisher = {Wiley},
  year      = {2015},
  url       = {http://repo.darmajaya.ac.id/4781/1/Time%20Series%20Analysis_%20Forecasting%20and%20Control%20%28PDFDrive%29.pdf}
}

@book{hyndman2018,
  author    = {Rob J. Hyndman and George Athanasopoulos},
  title     = {Forecasting: Principles and Practice},
  edition   = {2nd},
  publisher = {OTexts},
  year      = {2018},
  url       = {https://otexts.com/fpp2/}
}


@article{Tolo,
title = {Predicting systemic financial crises with recurrent neural networks},
journal = {Journal of Financial Stability},
volume = {49},
pages = {100746},
year = {2020},
issn = {1572-3089},
doi = {https://doi.org/10.1016/j.jfs.2020.100746},
url = {https://www.sciencedirect.com/science/article/pii/S1572308920300243},
author = {Eero Tölö},
keywords = {Early warning system, Systemic Banking crises, Neural networks, Validation},
abstract = {We consider predicting systemic financial crises one to five years ahead using recurrent neural networks. We evaluate the prediction performance with the Jórda-Schularick-Taylor dataset, which includes the crisis dates and annual macroeconomic series of 17 countries over the period 1870−2016. Previous literature has found that simple neural net architectures are useful and outperform the traditional logistic regression model in predicting systemic financial crises. We show that such predictions can be significantly improved by making use of the Long-Short Term Memory (RNN-LSTM) and the Gated Recurrent Unit (RNN-GRU) neural nets. Behind the success is the recurrent networks’ ability to make more robust predictions from the time series data. The results remain robust after extensive sensitivity analysis.}
}

@article{Hansika,
title = {Recurrent Neural Networks for Time Series Forecasting: Current status and future directions},
journal = {International Journal of Forecasting},
volume = {37},
number = {1},
pages = {388-427},
year = {2021},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2020.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0169207020300996},
author = {Hansika Hewamalage and Christoph Bergmeir and Kasun Bandara},
keywords = {Big data, Forecasting, Best practices, Framework},
abstract = {Recurrent Neural Networks (RNNs) have become competitive forecasting methods, as most notably shown in the winning method of the recent M4 competition. However, established statistical models such as exponential smoothing (ETS) and the autoregressive integrated moving average (ARIMA) gain their popularity not only from their high accuracy, but also because they are suitable for non-expert users in that they are robust, efficient, and automatic. In these areas, RNNs have still a long way to go. We present an extensive empirical study and an open-source software framework of existing RNN architectures for forecasting, and we develop guidelines and best practices for their use. For example, we conclude that RNNs are capable of modelling seasonality directly if the series in the dataset possess homogeneous seasonal patterns; otherwise, we recommend a deseasonalisation step. Comparisons against ETS and ARIMA demonstrate that (semi-) automatic RNN models are not silver bullets, but they are nevertheless competitive alternatives in many situations.}
}

@article{Fonville,
title ={Understanding Stock Market Corrections and Crashes (2024)},
year={2024},
url={https://www.covenantwealthadvisors.com/post/understanding-stock-market-corrections-and-crashes?utm_source=chatgpt.com},
author={Mark Fonville},
}

@article{Investo,
title={Guide to Stock Market Crash},
author={James Chen},
year={2022},
url={https://www.investopedia.com/terms/s/stock-market-crash.asp},
}

@book{hyndman2018,
   author = {Hyndman, Rob J. and Athanasopoulos, George},
   title = {Forecasting: Principles and Practice},
   edition = {2nd},
   publisher = {OTexts},
   year = {2018},
   url = {https://otexts.com/fpp2/}
}

@article{lim2021temporal,
   author = {Lim, Bryan and Arik, Sercan O. and Loeff, Nico and Pfister, Tomas},
   title = {Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting},
   journal = {International Journal of Forecasting},
   volume = {37},
   number = {4},
   pages = {1748--1764},
   year = {2021},
   doi = {10.1016/j.ijforecast.2021.03.012},
   url = {https://doi.org/10.1016/j.ijforecast.2021.03.012}
}

@article{vaswani2017attention,
   title={Attention Is All You Need},
   author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
   journal={arXiv preprint arXiv:1706.03762},
   year={2017},
   url={https://arxiv.org/abs/1706.03762}
}

@article{zhou2021informer,
   title={Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting},
   author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqiang and Zhang, Shuai and Li, Jianhui and Xiong, Hongkai and Zhang, Wancai},
   journal={Proceedings of the AAAI Conference on Artificial Intelligence},
   volume={35},
   number={12},
   pages={11106--11115},
   year={2021},
   url={https://ojs.aaai.org/index.php/AAAI/article/view/17325}
}

@article{lim2021temporal,
   title={Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting},
   author={Lim, Bryan and Arik, Sercan O and Loeff, Nico and Pfister, Tomas},
   journal={International Journal of Forecasting},
   volume={37},
   number={4},
   pages={1748-1764},
   year={2021},
   url={https://doi.org/10.1016/j.ijforecast.2021.03.012}
}

@article{zeng2023financial,
   title={Financial Time Series Forecasting using CNN and Transformer},
   author={Zeng, Zhen and others},
   journal={arXiv preprint arXiv:2304.04912},
   year={2023},
   url={https://arxiv.org/abs/2304.04912}
}

@article{sp500arimalstmregression, 
title={Prediction of SP500 Stock Index Using ARIM and Linear Regression}, 
volume={38}, url={https://drpress.org/ojs/index.php/HSET/article/view/5848}, DOI={10.54097/hset.v38i.5848}, 
abstractNote={This paper mainly establishes a linear model suitable for the volatility of the S&P 500 index and forecasts the S&P 500 index. Firstly, the data set is divided into the training set and test set. After testing a series of data attributes such as the smoothness of the original data series and log series, the original data series and log series of the S&P 500 index weekly data series are modeled based on the ARIMA model. The next step is to check the fit of the model and use ACF and PACF to determine the parameters of two different models to fit the original data series and the log data series, respectively. Based on two different models, the rationality of the model is confirmed by the residual white noise test and various natural maps. By establishing the model and analyzing the residual error of the model, finding out unreasonable fluctuation of the residual error of the model fitting and giving the corresponding explanation combined with the history. Finally, a fitted model to make rough forecasts for the S&P 500 from January 2020 to December 2020. Although this forecasting model cannot predict detailed fluctuations daily, it can still correctly determine whether a stock is going up or down. To sum up, the ARIMA model does not perform well in stock forecasting and it may need to be improved using other methods.}, journal={Highlights in Science, Engineering and Technology}, 
author={Guo, Kefei and Jiang, Zifan and Zhang, Yujian}, year={2023}, 
month={3}, 
pages={399–407} }

@article{hochreiter1997,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = {Long Short-Term Memory},
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}

@article{greff2017,
   title={LSTM: A Search Space Odyssey},
   volume={28},
   ISSN={2162-2388},
   url={http://dx.doi.org/10.1109/TNNLS.2016.2582924},
   DOI={10.1109/tnnls.2016.2582924},
   number={10},
   journal={IEEE Transactions on Neural Networks and Learning Systems},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Greff, Klaus and Srivastava, Rupesh K. and Koutnik, Jan and Steunebrink, Bas R. and Schmidhuber, Jurgen},
   year={2017},
   month=oct, pages={2222–2232} }
